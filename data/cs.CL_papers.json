[
  {
    "id": "2508.13152v1",
    "title": "RepreGuard: Detecting LLM-Generated Text by Revealing Hidden Representation Patterns",
    "authors": [
      "Xin Chen",
      "Junchao Wu",
      "Shu Yang",
      "Runzhe Zhan",
      "Zeyu Wu",
      "Ziyang Luo",
      "Di Wang",
      "Min Yang",
      "Lidia S. Chao",
      "Derek F. Wong"
    ],
    "published": "2025-08-18",
    "updated": "2025-08-18",
    "summary": "Detecting content generated by large language models (LLMs) is crucial for\npreventing misuse and building trustworthy AI systems. Although existing\ndetection methods perform well, their robustness in out-of-distribution (OOD)\nscenarios is still lacking. In this paper, we hypothesize that, compared to\nfeatures used by existing detection methods, the internal representations of\nLLMs contain more comprehensive and raw features that can more effectively\ncapture and distinguish the statistical pattern differences between\nLLM-generated texts (LGT) and human-written texts (HWT). We validated this\nhypothesis across different LLMs and observed significant differences in neural\nactivation patterns when processing these two types of texts. Based on this, we\npropose RepreGuard, an efficient statistics-based detection method.\nSpecifically, we first employ a surrogate model to collect representation of\nLGT and HWT, and extract the distinct activation feature that can better\nidentify LGT. We can classify the text by calculating the projection score of\nthe text representations along this feature direction and comparing with a\nprecomputed threshold. Experimental results show that RepreGuard outperforms\nall baselines with average 94.92% AUROC on both in-distribution (ID) and OOD\nscenarios, while also demonstrating robust resilience to various text sizes and\nmainstream attacks. Data and code are publicly available at:\nhttps://github.com/NLP2CT/RepreGuard",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "doi": null,
    "pdf_url": "http://arxiv.org/pdf/2508.13152v1",
    "comment": "Accepted to TACL 2025. This version is a pre-MIT Press publication\n  version",
    "journal_ref": null,
    "primary_category": "cs.CL",
    "links": [
      "http://arxiv.org/abs/2508.13152v1",
      "http://arxiv.org/pdf/2508.13152v1"
    ],
    "pdf": null
  },
  {
    "id": "2508.13144v1",
    "title": "Signal and Noise: A Framework for Reducing Uncertainty in Language Model Evaluation",
    "authors": [
      "David Heineman",
      "Valentin Hofmann",
      "Ian Magnusson",
      "Yuling Gu",
      "Noah A. Smith",
      "Hannaneh Hajishirzi",
      "Kyle Lo",
      "Jesse Dodge"
    ],
    "published": "2025-08-18",
    "updated": "2025-08-18",
    "summary": "Developing large language models is expensive and involves making decisions\nwith small experiments, typically by evaluating on large, multi-task evaluation\nsuites. In this work, we analyze specific properties which make a benchmark\nmore reliable for such decisions, and interventions to design higher-quality\nevaluation benchmarks. We introduce two key metrics that show differences in\ncurrent benchmarks: signal, a benchmark's ability to separate better models\nfrom worse models, and noise, a benchmark's sensitivity to random variability\nbetween training steps. We demonstrate that benchmarks with a better\nsignal-to-noise ratio are more reliable when making decisions at small scale,\nand those with less noise have lower scaling law prediction error. These\nresults suggest that improving signal or noise will lead to more useful\nbenchmarks, so we introduce three interventions designed to directly affect\nsignal or noise. For example, we propose that switching to a metric that has\nbetter signal and noise (e.g., perplexity rather than accuracy) leads to better\nreliability and improved scaling law error. We also find that filtering noisy\nsubtasks, to improve an aggregate signal-to-noise ratio, leads to more reliable\nmulti-task evaluations. We also find that averaging the output of a model's\nintermediate checkpoints to reduce noise leads to consistent improvements. We\nconclude by recommending that those creating new benchmarks, or selecting which\nexisting benchmarks to use, aim for high signal and low noise. We use 30\nbenchmarks for these experiments, and 375 open-weight language models from 60M\nto 32B parameters, resulting in a new, publicly available dataset of 900K\nevaluation benchmark results, totaling 200M instances.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "doi": null,
    "pdf_url": "http://arxiv.org/pdf/2508.13144v1",
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CL",
    "links": [
      "http://arxiv.org/abs/2508.13144v1",
      "http://arxiv.org/pdf/2508.13144v1"
    ],
    "pdf": null
  },
  {
    "id": "2508.13142v1",
    "title": "Has GPT-5 Achieved Spatial Intelligence? An Empirical Study",
    "authors": [
      "Zhongang Cai",
      "Yubo Wang",
      "Qingping Sun",
      "Ruisi Wang",
      "Chenyang Gu",
      "Wanqi Yin",
      "Zhiqian Lin",
      "Zhitao Yang",
      "Chen Wei",
      "Xuanke Shi",
      "Kewang Deng",
      "Xiaoyang Han",
      "Zukai Chen",
      "Jiaqi Li",
      "Xiangyu Fan",
      "Hanming Deng",
      "Lewei Lu",
      "Bo Li",
      "Ziwei Liu",
      "Quan Wang",
      "Dahua Lin",
      "Lei Yang"
    ],
    "published": "2025-08-18",
    "updated": "2025-08-18",
    "summary": "Multi-modal models have achieved remarkable progress in recent years.\nNevertheless, they continue to exhibit notable limitations in spatial\nunderstanding and reasoning, which are fundamental capabilities to achieving\nartificial general intelligence. With the recent release of GPT-5, allegedly\nthe most powerful AI model to date, it is timely to examine where the leading\nmodels stand on the path toward spatial intelligence. First, we propose a\ncomprehensive taxonomy of spatial tasks that unifies existing benchmarks and\ndiscuss the challenges in ensuring fair evaluation. We then evaluate\nstate-of-the-art proprietary and open-source models on eight key benchmarks, at\na cost exceeding one billion total tokens. Our empirical study reveals that (1)\nGPT-5 demonstrates unprecedented strength in spatial intelligence, yet (2)\nstill falls short of human performance across a broad spectrum of tasks.\nMoreover, we (3) identify the more challenging spatial intelligence problems\nfor multi-modal models, and (4) proprietary models do not exhibit a decisive\nadvantage when facing the most difficult problems. In addition, we conduct a\nqualitative evaluation across a diverse set of scenarios that are intuitive for\nhumans yet fail even the most advanced multi-modal models.",
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.LG",
      "cs.MM",
      "cs.RO"
    ],
    "doi": null,
    "pdf_url": "http://arxiv.org/pdf/2508.13142v1",
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV",
    "links": [
      "http://arxiv.org/abs/2508.13142v1",
      "http://arxiv.org/pdf/2508.13142v1"
    ],
    "pdf": null
  },
  {
    "id": "2508.13141v1",
    "title": "OptimalThinkingBench: Evaluating Over and Underthinking in LLMs",
    "authors": [
      "Pranjal Aggarwal",
      "Seungone Kim",
      "Jack Lanchantin",
      "Sean Welleck",
      "Jason Weston",
      "Ilia Kulikov",
      "Swarnadeep Saha"
    ],
    "published": "2025-08-18",
    "updated": "2025-08-18",
    "summary": "Thinking LLMs solve complex tasks at the expense of increased compute and\noverthinking on simpler problems, while non-thinking LLMs are faster and\ncheaper but underthink on harder reasoning problems. This has led to the\ndevelopment of separate thinking and non-thinking LLM variants, leaving the\nonus of selecting the optimal model for each query on the end user. In this\nwork, we introduce OptimalThinkingBench, a unified benchmark that jointly\nevaluates overthinking and underthinking in LLMs and also encourages the\ndevelopment of optimally-thinking models that balance performance and\nefficiency. Our benchmark comprises two sub-benchmarks: OverthinkingBench,\nfeaturing simple queries in 72 domains, and UnderthinkingBench, containing 11\nchallenging reasoning tasks. Using novel thinking-adjusted accuracy metrics, we\nperform extensive evaluation of 33 different thinking and non-thinking models\nand show that no model is able to optimally think on our benchmark. Thinking\nmodels often overthink for hundreds of tokens on the simplest user queries\nwithout improving performance. In contrast, large non-thinking models\nunderthink, often falling short of much smaller thinking models. We further\nexplore several methods to encourage optimal thinking, but find that these\napproaches often improve on one sub-benchmark at the expense of the other,\nhighlighting the need for better unified and optimal models in the future.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "doi": null,
    "pdf_url": "http://arxiv.org/pdf/2508.13141v1",
    "comment": "26 pages, 6 tables, 10 figures",
    "journal_ref": null,
    "primary_category": "cs.CL",
    "links": [
      "http://arxiv.org/abs/2508.13141v1",
      "http://arxiv.org/pdf/2508.13141v1"
    ],
    "pdf": null
  },
  {
    "id": "2508.13131v1",
    "title": "Improving Detection of Watermarked Language Models",
    "authors": [
      "Dara Bahri",
      "John Wieting"
    ],
    "published": "2025-08-18",
    "updated": "2025-08-18",
    "summary": "Watermarking has recently emerged as an effective strategy for detecting the\ngenerations of large language models (LLMs). The strength of a watermark\ntypically depends strongly on the entropy afforded by the language model and\nthe set of input prompts. However, entropy can be quite limited in practice,\nespecially for models that are post-trained, for example via instruction tuning\nor reinforcement learning from human feedback (RLHF), which makes detection\nbased on watermarking alone challenging. In this work, we investigate whether\ndetection can be improved by combining watermark detectors with non-watermark\nones. We explore a number of hybrid schemes that combine the two, observing\nperformance gains over either class of detector under a wide range of\nexperimental conditions.",
    "categories": [
      "cs.CL",
      "cs.LG",
      "stat.ML"
    ],
    "doi": null,
    "pdf_url": "http://arxiv.org/pdf/2508.13131v1",
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CL",
    "links": [
      "http://arxiv.org/abs/2508.13131v1",
      "http://arxiv.org/pdf/2508.13131v1"
    ],
    "pdf": null
  },
  {
    "id": "2508.13130v1",
    "title": "MuDRiC: Multi-Dialect Reasoning for Arabic Commonsense Validation",
    "authors": [
      "Kareem Elozeiri",
      "Mervat Abassy",
      "Preslav Nakov",
      "Yuxia Wang"
    ],
    "published": "2025-08-18",
    "updated": "2025-08-18",
    "summary": "Commonsense validation evaluates whether a sentence aligns with everyday\nhuman understanding, a critical capability for developing robust natural\nlanguage understanding systems. While substantial progress has been made in\nEnglish, the task remains underexplored in Arabic, particularly given its rich\nlinguistic diversity. Existing Arabic resources have primarily focused on\nModern Standard Arabic (MSA), leaving regional dialects underrepresented\ndespite their prevalence in spoken contexts. To bridge this gap, we present two\nkey contributions: (i) we introduce MuDRiC, an extended Arabic commonsense\ndataset incorporating multiple dialects, and (ii) a novel method adapting Graph\nConvolutional Networks (GCNs) to Arabic commonsense reasoning, which enhances\nsemantic relationship modeling for improved commonsense validation. Our\nexperimental results demonstrate that this approach achieves superior\nperformance in Arabic commonsense validation. Our work enhances Arabic natural\nlanguage understanding by providing both a foundational dataset and a novel\nmethod for handling its complex variations. To the best of our knowledge, we\nrelease the first Arabic multi-dialect commonsense reasoning dataset.",
    "categories": [
      "cs.CL"
    ],
    "doi": null,
    "pdf_url": "http://arxiv.org/pdf/2508.13130v1",
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CL",
    "links": [
      "http://arxiv.org/abs/2508.13130v1",
      "http://arxiv.org/pdf/2508.13130v1"
    ],
    "pdf": null
  },
  {
    "id": "2508.13124v1",
    "title": "Spot the BlindSpots: Systematic Identification and Quantification of Fine-Grained LLM Biases in Contact Center Summaries",
    "authors": [
      "Kawin Mayilvaghanan",
      "Siddhant Gupta",
      "Ayush Kumar"
    ],
    "published": "2025-08-18",
    "updated": "2025-08-18",
    "summary": "Abstractive summarization is a core application in contact centers, where\nLarge Language Models (LLMs) generate millions of summaries of call transcripts\ndaily. Despite their apparent quality, it remains unclear whether LLMs\nsystematically under- or over-attend to specific aspects of the transcript,\npotentially introducing biases in the generated summary. While prior work has\nexamined social and positional biases, the specific forms of bias pertinent to\ncontact center operations - which we term Operational Bias - have remained\nunexplored. To address this gap, we introduce BlindSpot, a framework built upon\na taxonomy of 15 operational bias dimensions (e.g., disfluency, speaker, topic)\nfor the identification and quantification of these biases. BlindSpot leverages\nan LLM as a zero-shot classifier to derive categorical distributions for each\nbias dimension in a pair of transcript and its summary. The bias is then\nquantified using two metrics: Fidelity Gap (the JS Divergence between\ndistributions) and Coverage (the percentage of source labels omitted). Using\nBlindSpot, we conducted an empirical study with 2500 real call transcripts and\ntheir summaries generated by 20 LLMs of varying scales and families (e.g., GPT,\nLlama, Claude). Our analysis reveals that biases are systemic and present\nacross all evaluated models, regardless of size or family.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "doi": null,
    "pdf_url": "http://arxiv.org/pdf/2508.13124v1",
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CL",
    "links": [
      "http://arxiv.org/abs/2508.13124v1",
      "http://arxiv.org/pdf/2508.13124v1"
    ],
    "pdf": null
  },
  {
    "id": "2508.13118v1",
    "title": "AutoBnB-RAG: Enhancing Multi-Agent Incident Response with Retrieval-Augmented Generation",
    "authors": [
      "Zefang Liu",
      "Arman Anwar"
    ],
    "published": "2025-08-18",
    "updated": "2025-08-18",
    "summary": "Incident response (IR) requires fast, coordinated, and well-informed\ndecision-making to contain and mitigate cyber threats. While large language\nmodels (LLMs) have shown promise as autonomous agents in simulated IR settings,\ntheir reasoning is often limited by a lack of access to external knowledge. In\nthis work, we present AutoBnB-RAG, an extension of the AutoBnB framework that\nincorporates retrieval-augmented generation (RAG) into multi-agent incident\nresponse simulations. Built on the Backdoors & Breaches (B&B) tabletop game\nenvironment, AutoBnB-RAG enables agents to issue retrieval queries and\nincorporate external evidence during collaborative investigations. We introduce\ntwo retrieval settings: one grounded in curated technical documentation\n(RAG-Wiki), and another using narrative-style incident reports (RAG-News). We\nevaluate performance across eight team structures, including newly introduced\nargumentative configurations designed to promote critical reasoning. To\nvalidate practical utility, we also simulate real-world cyber incidents based\non public breach reports, demonstrating AutoBnB-RAG's ability to reconstruct\ncomplex multi-stage attacks. Our results show that retrieval augmentation\nimproves decision quality and success rates across diverse organizational\nmodels. This work demonstrates the value of integrating retrieval mechanisms\ninto LLM-based multi-agent systems for cybersecurity decision-making.",
    "categories": [
      "cs.CL",
      "cs.CR"
    ],
    "doi": null,
    "pdf_url": "http://arxiv.org/pdf/2508.13118v1",
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CL",
    "links": [
      "http://arxiv.org/abs/2508.13118v1",
      "http://arxiv.org/pdf/2508.13118v1"
    ],
    "pdf": null
  },
  {
    "id": "2508.13107v1",
    "title": "All for law and law for all: Adaptive RAG Pipeline for Legal Research",
    "authors": [
      "Figarri Keisha",
      "Prince Singh",
      "Pallavi",
      "Dion Fernandes",
      "Aravindh Manivannan",
      "Ilham Wicaksono",
      "Faisal Ahmad"
    ],
    "published": "2025-08-18",
    "updated": "2025-08-18",
    "summary": "Retrieval-Augmented Generation (RAG) mitigates hallucinations by grounding\nlarge language model outputs in cited sources, a capability that is especially\ncritical in the legal domain. We present an end-to-end RAG pipeline that\nrevisits and extends the LegalBenchRAG baseline with three targeted\nenhancements: (i) a context-aware query translator that disentangles document\nreferences from natural-language questions and adapts retrieval depth and\nresponse style based on expertise and specificity, (ii) open-source retrieval\nstrategies using SBERT and GTE embeddings that achieve substantial performance\ngains (improving Recall@K by 30-95\\% and Precision@K by $\\sim$2.5$\\times$ for\n$K>4$) while remaining cost-efficient, and (iii) a comprehensive evaluation and\ngeneration framework that combines RAGAS, BERTScore-F1, and ROUGE-Recall to\nassess semantic alignment and faithfulness across models and prompt designs.\nOur results show that carefully designed open-source pipelines can rival or\noutperform proprietary approaches in retrieval quality, while a custom\nlegal-grounded prompt consistently produces more faithful and contextually\nrelevant answers than baseline prompting. Taken together, these contributions\ndemonstrate the potential of task-aware, component-level tuning to deliver\nlegally grounded, reproducible, and cost-effective RAG systems for legal\nresearch assistance.",
    "categories": [
      "cs.CL",
      "cs.IR",
      "F.2.2, H.3.3, I.2.7"
    ],
    "doi": null,
    "pdf_url": "http://arxiv.org/pdf/2508.13107v1",
    "comment": "submitted to NLLP 2025 Workshop",
    "journal_ref": null,
    "primary_category": "cs.CL",
    "links": [
      "http://arxiv.org/abs/2508.13107v1",
      "http://arxiv.org/pdf/2508.13107v1"
    ],
    "pdf": null
  },
  {
    "id": "2508.13079v1",
    "title": "DocHPLT: A Massively Multilingual Document-Level Translation Dataset",
    "authors": [
      "Dayyán O'Brien",
      "Bhavitvya Malik",
      "Ona de Gibert",
      "Pinzhen Chen",
      "Barry Haddow",
      "Jörg Tiedemann"
    ],
    "published": "2025-08-18",
    "updated": "2025-08-18",
    "summary": "Existing document-level machine translation resources are only available for\na handful of languages, mostly high-resourced ones. To facilitate the training\nand evaluation of document-level translation and, more broadly, long-context\nmodeling for global communities, we create DocHPLT, the largest publicly\navailable document-level translation dataset to date. It contains 124 million\naligned document pairs across 50 languages paired with English, comprising 4.26\nbillion sentences, with further possibility to provide 2500 bonus pairs not\ninvolving English. Unlike previous reconstruction-based approaches that piece\ntogether documents from sentence-level data, we modify an existing web\nextraction pipeline to preserve complete document integrity from the source,\nretaining all content including unaligned portions. After our preliminary\nexperiments identify the optimal training context strategy for document-level\ntranslation, we demonstrate that LLMs fine-tuned on DocHPLT substantially\noutperform off-the-shelf instruction-tuned baselines, with particularly\ndramatic improvements for under-resourced languages. We open-source the dataset\nunder a permissive license, providing essential infrastructure for advancing\nmultilingual document-level translation.",
    "categories": [
      "cs.CL"
    ],
    "doi": null,
    "pdf_url": "http://arxiv.org/pdf/2508.13079v1",
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CL",
    "links": [
      "http://arxiv.org/abs/2508.13079v1",
      "http://arxiv.org/pdf/2508.13079v1"
    ],
    "pdf": null
  },
  {
    "id": "2508.13070v1",
    "title": "Reinforced Context Order Recovery for Adaptive Reasoning and Planning",
    "authors": [
      "Long Ma",
      "Fangwei Zhong",
      "Yizhou Wang"
    ],
    "published": "2025-08-18",
    "updated": "2025-08-18",
    "summary": "Modern causal language models, followed by rapid developments in discrete\ndiffusion models, can now produce a wide variety of interesting and useful\ncontent. However, these families of models are predominantly trained to output\ntokens with a fixed (left-to-right) or random order, which may deviate from the\nlogical order in which tokens are generated originally. In this paper, we\nobserve that current causal and diffusion models encounter difficulties in\nproblems that require adaptive token generation orders to solve tractably,\nwhich we characterize with the $\\mathcal{V}$-information framework. Motivated\nby this, we propose Reinforced Context Order Recovery (ReCOR), a\nreinforcement-learning-based framework to extract adaptive, data-dependent\ntoken generation orders from text data without annotations. Self-supervised by\ntoken prediction statistics, ReCOR estimates the hardness of predicting every\nunfilled token and adaptively selects the next token during both training and\ninference. Experiments on challenging reasoning and planning datasets\ndemonstrate the superior performance of ReCOR compared with baselines,\nsometimes outperforming oracle models supervised with the ground-truth order.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "doi": null,
    "pdf_url": "http://arxiv.org/pdf/2508.13070v1",
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CL",
    "links": [
      "http://arxiv.org/abs/2508.13070v1",
      "http://arxiv.org/pdf/2508.13070v1"
    ],
    "pdf": null
  },
  {
    "id": "2508.13060v1",
    "title": "Evaluating ASR robustness to spontaneous speech errors: A study of WhisperX using a Speech Error Database",
    "authors": [
      "John Alderete",
      "Macarious Kin Fung Hui",
      "Aanchan Mohan"
    ],
    "published": "2025-08-18",
    "updated": "2025-08-18",
    "summary": "The Simon Fraser University Speech Error Database (SFUSED) is a public data\ncollection developed for linguistic and psycholinguistic research. Here we\ndemonstrate how its design and annotations can be used to test and evaluate\nspeech recognition models. The database comprises systematically annotated\nspeech errors from spontaneous English speech, with each error tagged for\nintended and actual error productions. The annotation schema incorporates\nmultiple classificatory dimensions that are of some value to model assessment,\nincluding linguistic hierarchical level, contextual sensitivity, degraded\nwords, word corrections, and both word-level and syllable-level error\npositioning. To assess the value of these classificatory variables, we\nevaluated the transcription accuracy of WhisperX across 5,300 documented word\nand phonological errors. This analysis demonstrates the atabase's effectiveness\nas a diagnostic tool for ASR system performance.",
    "categories": [
      "cs.CL"
    ],
    "doi": null,
    "pdf_url": "http://arxiv.org/pdf/2508.13060v1",
    "comment": "5 pages, 6 figures, 1 table, Interspeech 2025 (Rotterdam)",
    "journal_ref": null,
    "primary_category": "cs.CL",
    "links": [
      "http://arxiv.org/abs/2508.13060v1",
      "http://arxiv.org/pdf/2508.13060v1"
    ],
    "pdf": null
  },
  {
    "id": "2508.13058v1",
    "title": "Doğal Dil İşlemede Tokenizasyon Standartları ve Ölçümü: Türkçe Üzerinden Büyük Dil Modellerinin Karşılaştırmalı Analizi",
    "authors": [
      "M. Ali Bayram",
      "Ali Arda Fincan",
      "Ahmet Semih Gümüş",
      "Sercan Karakaş",
      "Banu Diri",
      "Savaş Yıldırım"
    ],
    "published": "2025-08-18",
    "updated": "2025-08-18",
    "summary": "Tokenization is a fundamental preprocessing step in Natural Language\nProcessing (NLP), significantly impacting the capability of large language\nmodels (LLMs) to capture linguistic and semantic nuances. This study introduces\na novel evaluation framework addressing tokenization challenges specific to\nmorphologically-rich and low-resource languages such as Turkish. Utilizing the\nTurkish MMLU (TR-MMLU) dataset, comprising 6,200 multiple-choice questions from\nthe Turkish education system, we assessed tokenizers based on vocabulary size,\ntoken count, processing time, language-specific token percentages (\\%TR), and\ntoken purity (\\%Pure). These newly proposed metrics measure how effectively\ntokenizers preserve linguistic structures. Our analysis reveals that\nlanguage-specific token percentages exhibit a stronger correlation with\ndownstream performance (e.g., MMLU scores) than token purity. Furthermore,\nincreasing model parameters alone does not necessarily enhance linguistic\nperformance, underscoring the importance of tailored, language-specific\ntokenization methods. The proposed framework establishes robust and practical\ntokenization standards for morphologically complex languages.",
    "categories": [
      "cs.CL",
      "68T50",
      "I.2.7; I.2.6"
    ],
    "doi": "10.1109/SIU66497.2025.11112220",
    "pdf_url": "http://arxiv.org/pdf/2508.13058v1",
    "comment": "in Turkish language, Presented at the 2025 33rd Signal Processing and\n  Communications Applications Conference (SIU), 25--28 June 2025, \\c{S}ile,\n  Istanbul, T\\\"urkiye",
    "journal_ref": null,
    "primary_category": "cs.CL",
    "links": [
      "http://dx.doi.org/10.1109/SIU66497.2025.11112220",
      "http://arxiv.org/abs/2508.13058v1",
      "http://arxiv.org/pdf/2508.13058v1"
    ],
    "pdf": null
  },
  {
    "id": "2508.13044v1",
    "title": "Büyük Dil Modelleri için TR-MMLU Benchmarkı: Performans Değerlendirmesi, Zorluklar ve İyileştirme Fırsatları",
    "authors": [
      "M. Ali Bayram",
      "Ali Arda Fincan",
      "Ahmet Semih Gümüş",
      "Banu Diri",
      "Savaş Yıldırım",
      "Öner Aytaş"
    ],
    "published": "2025-08-18",
    "updated": "2025-08-18",
    "summary": "Language models have made significant advancements in understanding and\ngenerating human language, achieving remarkable success in various\napplications. However, evaluating these models remains a challenge,\nparticularly for resource-limited languages like Turkish. To address this\nissue, we introduce the Turkish MMLU (TR-MMLU) benchmark, a comprehensive\nevaluation framework designed to assess the linguistic and conceptual\ncapabilities of large language models (LLMs) in Turkish. TR-MMLU is based on a\nmeticulously curated dataset comprising 6,200 multiple-choice questions across\n62 sections within the Turkish education system. This benchmark provides a\nstandard framework for Turkish NLP research, enabling detailed analyses of\nLLMs' capabilities in processing Turkish text. In this study, we evaluated\nstate-of-the-art LLMs on TR-MMLU, highlighting areas for improvement in model\ndesign. TR-MMLU sets a new standard for advancing Turkish NLP research and\ninspiring future innovations.",
    "categories": [
      "cs.CL",
      "68T50",
      "I.2.7; I.2.6"
    ],
    "doi": "10.1109/SIU66497.2025.11112154",
    "pdf_url": "http://arxiv.org/pdf/2508.13044v1",
    "comment": "10 pages, in Turkish language, 5 figures. Presented at the 2025 33rd\n  Signal Processing and Communications Applications Conference (SIU), 25--28\n  June 2025, Sile, Istanbul, T\\\"urkiye",
    "journal_ref": null,
    "primary_category": "cs.CL",
    "links": [
      "http://dx.doi.org/10.1109/SIU66497.2025.11112154",
      "http://arxiv.org/abs/2508.13044v1",
      "http://arxiv.org/pdf/2508.13044v1"
    ],
    "pdf": null
  },
  {
    "id": "2508.13037v1",
    "title": "Can Large Models Teach Student Models to Solve Mathematical Problems Like Human Beings? A Reasoning Distillation Method via Multi-LoRA Interaction",
    "authors": [
      "Xinhe Li",
      "Jiajun Liu",
      "Peng Wang"
    ],
    "published": "2025-08-18",
    "updated": "2025-08-18",
    "summary": "Recent studies have demonstrated that Large Language Models (LLMs) have\nstrong mathematical reasoning abilities but rely on hundreds of billions of\nparameters. To tackle the challenge of poor reasoning in Small Language Models\n(SLMs), existing methods typically leverage LLMs to generate massive amounts of\ndata for cramming training. In psychology, they are akin to System 1 thinking,\nwhich resolves reasoning problems rapidly based on experience and intuition.\nHowever, human learning also requires System 2 thinking, where knowledge is\nfirst acquired and then reinforced through practice. Inspired by such two\ndistinct modes of thinking, we propose a novel method based on the multi-LoRA\nInteraction for mathematical reasoning Distillation (LoRID). First, we input\nthe question and reasoning of each sample into an LLM to create\nknowledge-enhanced datasets. Subsequently, we train a LoRA block on the student\nmodel as an Intuitive Reasoner (IR), which directly generates Chain-of-Thoughts\nfor problem-solving. Then, to imitate System 2 thinking, we train the Knowledge\nGenerator (KG) and Deep Reasoner (DR), respectively. The former outputs only\nknowledge after receiving problems, while the latter uses that knowledge to\nperform reasoning. Finally, to address the randomness in the generation of IR\nand DR, we evaluate whether their outputs are consistent, and the inference\nprocess needs to be iterated if not. This step can enhance the mathematical\nreasoning ability of SLMs through mutual feedback. Experimental results show\nthat LoRID achieves state-of-the-art performance, especially on the GSM8K\ndataset, where it outperforms the second-best method by 2.3%, 16.1%, 2.4%,\n12.3%, and 1.8% accuracy across the five base models, respectively.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "doi": null,
    "pdf_url": "http://arxiv.org/pdf/2508.13037v1",
    "comment": "Accepted by IJCAI2025",
    "journal_ref": null,
    "primary_category": "cs.CL",
    "links": [
      "http://arxiv.org/abs/2508.13037v1",
      "http://arxiv.org/pdf/2508.13037v1"
    ],
    "pdf": null
  },
  {
    "id": "2508.13028v1",
    "title": "Integrating Feedback Loss from Bi-modal Sarcasm Detector for Sarcastic Speech Synthesis",
    "authors": [
      "Zhu Li",
      "Yuqing Zhang",
      "Xiyuan Gao",
      "Devraj Raghuvanshi",
      "Nagendra Kumar",
      "Shekhar Nayak",
      "Matt Coler"
    ],
    "published": "2025-08-18",
    "updated": "2025-08-18",
    "summary": "Sarcastic speech synthesis, which involves generating speech that effectively\nconveys sarcasm, is essential for enhancing natural interactions in\napplications such as entertainment and human-computer interaction. However,\nsynthesizing sarcastic speech remains a challenge due to the nuanced prosody\nthat characterizes sarcasm, as well as the limited availability of annotated\nsarcastic speech data. To address these challenges, this study introduces a\nnovel approach that integrates feedback loss from a bi-modal sarcasm detection\nmodel into the TTS training process, enhancing the model's ability to capture\nand convey sarcasm. In addition, by leveraging transfer learning, a speech\nsynthesis model pre-trained on read speech undergoes a two-stage fine-tuning\nprocess. First, it is fine-tuned on a diverse dataset encompassing various\nspeech styles, including sarcastic speech. In the second stage, the model is\nfurther refined using a dataset focused specifically on sarcastic speech,\nenhancing its ability to generate sarcasm-aware speech. Objective and\nsubjective evaluations demonstrate that our proposed methods improve the\nquality, naturalness, and sarcasm-awareness of synthesized speech.",
    "categories": [
      "cs.CL"
    ],
    "doi": null,
    "pdf_url": "http://arxiv.org/pdf/2508.13028v1",
    "comment": "Speech Synthesis Workshop 2025",
    "journal_ref": null,
    "primary_category": "cs.CL",
    "links": [
      "http://arxiv.org/abs/2508.13028v1",
      "http://arxiv.org/pdf/2508.13028v1"
    ],
    "pdf": null
  },
  {
    "id": "2508.13024v1",
    "title": "WebMall -- A Multi-Shop Benchmark for Evaluating Web Agents",
    "authors": [
      "Ralph Peeters",
      "Aaron Steiner",
      "Luca Schwarz",
      "Julian Yuya Caspary",
      "Christian Bizer"
    ],
    "published": "2025-08-18",
    "updated": "2025-08-18",
    "summary": "LLM-based web agents have the potential to automate long-running web tasks,\nsuch as finding offers for specific products in multiple online shops and\nsubsequently ordering the cheapest products that meet the users needs. This\npaper introduces WebMall, a multi-shop online shopping benchmark for evaluating\nthe effectiveness and efficiency of web agents for comparison-shopping. WebMall\nconsists of four simulated online shops populated with authentic product offers\nsourced from the Common Crawl, alongside a suite of 91 cross-shop tasks. These\ntasks include basic tasks such as finding specific products in multiple shops,\nperforming price comparisons, adding items to the shopping cart, and completing\ncheckout. Advanced tasks involve searching for products based on vague\nrequirements, identifying suitable substitutes, and finding compatible\nproducts. Compared to existing e-commerce benchmarks, such as WebShop or\nShoppingBench, WebMall introduces comparison-shopping tasks across multiple\nshops. Furthermore, the product offers are more heterogeneous, as they\noriginate from hundreds of distinct real-world shops. The tasks in WebMall\nrequire longer interaction trajectories than those in WebShop, while remaining\nrepresentative of real-world shopping behaviors. We evaluate eight baseline\nagents on WebMall, varying in observation modality, memory utilization, and\nunderlying large language model (GPT 4.1 and Claude Sonnet 4). The\nbest-performing configurations achieve completion rates of 75% and 53%, and F1\nscores of 87% and 63%, on the basic and advanced task sets, respectively.\nWebMall is publicly released to facilitate research on web agents and to\npromote advancements in navigation, reasoning, and efficiency within e-commerce\nscenarios.",
    "categories": [
      "cs.CL"
    ],
    "doi": null,
    "pdf_url": "http://arxiv.org/pdf/2508.13024v1",
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CL",
    "links": [
      "http://arxiv.org/abs/2508.13024v1",
      "http://arxiv.org/pdf/2508.13024v1"
    ],
    "pdf": null
  },
  {
    "id": "2508.13021v1",
    "title": "PC-Sampler: Position-Aware Calibration of Decoding Bias in Masked Diffusion Models",
    "authors": [
      "Pengcheng Huang",
      "Shuhao Liu",
      "Zhenghao Liu",
      "Yukun Yan",
      "Shuo Wang",
      "Zulong Chen",
      "Tong Xiao"
    ],
    "published": "2025-08-18",
    "updated": "2025-08-18",
    "summary": "Recent advances in masked diffusion models (MDMs) have established them as\npowerful non-autoregressive alternatives for sequence generation. Nevertheless,\nour preliminary experiments reveal that the generation quality of MDMs is still\nhighly sensitive to the choice of decoding strategy. In particular, widely\nadopted uncertainty-based samplers suffer from two key limitations: a lack of\nglobal trajectory control and a pronounced bias toward trivial tokens in the\nearly stages of decoding. These shortcomings restrict the full potential of\nMDMs. In this work, we introduce Position-Aware Confidence-Calibrated Sampling\n(PC-Sampler), a novel decoding strategy that unifies global trajectory planning\nwith content-aware informativeness maximization. PC-Sampler incorporates a\nposition-aware weighting mechanism to regulate the decoding path and a\ncalibrated confidence score to suppress the premature selection of trivial\ntokens. Extensive experiments on three advanced MDMs across seven challenging\nbenchmarks-including logical reasoning and planning tasks-demonstrate that\nPC-Sampler consistently outperforms existing MDM decoding strategies by more\nthan 10% on average, significantly narrowing the performance gap with\nstate-of-the-art autoregressive models. All codes are available at\nhttps://github.com/NEUIR/PC-Sampler.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "doi": null,
    "pdf_url": "http://arxiv.org/pdf/2508.13021v1",
    "comment": "17 pages,13 figures",
    "journal_ref": null,
    "primary_category": "cs.AI",
    "links": [
      "http://arxiv.org/abs/2508.13021v1",
      "http://arxiv.org/pdf/2508.13021v1"
    ],
    "pdf": null
  },
  {
    "id": "2508.12981v1",
    "title": "Analyzing Information Sharing and Coordination in Multi-Agent Planning",
    "authors": [
      "Tianyue Ou",
      "Saujas Vaduguru",
      "Daniel Fried"
    ],
    "published": "2025-08-18",
    "updated": "2025-08-18",
    "summary": "Multi-agent systems (MASs) have pushed the boundaries of large language model\n(LLM) agents in domains such as web research and software engineering. However,\nlong-horizon, multi-constraint planning tasks involve conditioning on detailed\ninformation and satisfying complex interdependent constraints, which can pose a\nchallenge for these systems. In this study, we construct an LLM-based MAS for a\ntravel planning task which is representative of these challenges. We evaluate\nthe impact of a notebook to facilitate information sharing, and evaluate an\norchestrator agent to improve coordination in free form conversation between\nagents. We find that the notebook reduces errors due to hallucinated details by\n18%, while an orchestrator directs the MAS to focus on and further reduce\nerrors by up to 13.5% within focused sub-areas. Combining both mechanisms\nachieves a 25% final pass rate on the TravelPlanner benchmark, a 17.5% absolute\nimprovement over the single-agent baseline's 7.5% pass rate. These results\nhighlight the potential of structured information sharing and reflective\norchestration as key components in MASs for long horizon planning with LLMs.",
    "categories": [
      "cs.CL"
    ],
    "doi": null,
    "pdf_url": "http://arxiv.org/pdf/2508.12981v1",
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CL",
    "links": [
      "http://arxiv.org/abs/2508.12981v1",
      "http://arxiv.org/pdf/2508.12981v1"
    ],
    "pdf": null
  },
  {
    "id": "2508.12907v1",
    "title": "SNAP-UQ: Self-supervised Next-Activation Prediction for Single-Pass Uncertainty in TinyML",
    "authors": [
      "Ismail Lamaakal",
      "Chaymae Yahyati",
      "Khalid El Makkaoui",
      "Ibrahim Ouahbi",
      "Yassine Maleh"
    ],
    "published": "2025-08-18",
    "updated": "2025-08-18",
    "summary": "We introduce \\textbf{SNAP-UQ}, a single-pass, label-free uncertainty method\nfor TinyML that estimates risk from \\emph{depth-wise next-activation\nprediction}: tiny int8 heads forecast the statistics of the next layer from a\ncompressed view of the previous one, and a lightweight monotone mapper turns\nthe resulting surprisal into an actionable score. The design requires no\ntemporal buffers, auxiliary exits, or repeated forward passes, and adds only a\nfew tens of kilobytes to MCU deployments. Across vision and audio backbones,\nSNAP-UQ consistently reduces flash and latency relative to early-exit and deep\nensembles (typically $\\sim$40--60\\% smaller and $\\sim$25--35\\% faster), with\ncompeting methods of similar accuracy often exceeding memory limits. In\ncorrupted streams it improves accuracy-drop detection by several AUPRC points\nand maintains strong failure detection (AUROC $\\approx$0.9) in a single pass.\nGrounding uncertainty in layer-to-layer dynamics yields a practical,\nresource-efficient basis for on-device monitoring in TinyML.",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "doi": null,
    "pdf_url": "http://arxiv.org/pdf/2508.12907v1",
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.LG",
    "links": [
      "http://arxiv.org/abs/2508.12907v1",
      "http://arxiv.org/pdf/2508.12907v1"
    ],
    "pdf": null
  },
  {
    "id": "2508.12905v1",
    "title": "TCUQ: Single-Pass Uncertainty Quantification from Temporal Consistency with Streaming Conformal Calibration for TinyML",
    "authors": [
      "Ismail Lamaakal",
      "Chaymae Yahyati",
      "Khalid El Makkaoui",
      "Ibrahim Ouahbi",
      "Yassine Maleh"
    ],
    "published": "2025-08-18",
    "updated": "2025-08-18",
    "summary": "We introduce TCUQ, a single pass, label free uncertainty monitor for\nstreaming TinyML that converts short horizon temporal consistency captured via\nlightweight signals on posteriors and features into a calibrated risk score\nwith an O(W ) ring buffer and O(1) per step updates. A streaming conformal\nlayer turns this score into a budgeted accept/abstain rule, yielding calibrated\nbehavior without online labels or extra forward passes. On microcontrollers,\nTCUQ fits comfortably on kilobyte scale devices and reduces footprint and\nlatency versus early exit and deep ensembles (typically about 50 to 60% smaller\nand about 30 to 45% faster), while methods of similar accuracy often run out of\nmemory. Under corrupted in distribution streams, TCUQ improves accuracy drop\ndetection by 3 to 7 AUPRC points and reaches up to 0.86 AUPRC at high\nseverities; for failure detection it attains up to 0.92 AUROC. These results\nshow that temporal consistency, coupled with streaming conformal calibration,\nprovides a practical and resource efficient foundation for on device monitoring\nin TinyML.",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "doi": null,
    "pdf_url": "http://arxiv.org/pdf/2508.12905v1",
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.LG",
    "links": [
      "http://arxiv.org/abs/2508.12905v1",
      "http://arxiv.org/pdf/2508.12905v1"
    ],
    "pdf": null
  },
  {
    "id": "2508.12903v1",
    "title": "A Stitch in Time Saves Nine: Proactive Self-Refinement for Language Models",
    "authors": [
      "Jinyi Han",
      "Xinyi Wang",
      "Haiquan Zhao",
      "Tingyun li",
      "Zishang Jiang",
      "Sihang Jiang",
      "Jiaqing Liang",
      "Xin Lin",
      "Weikang Zhou",
      "Zeye Sun",
      "Fei Yu",
      "Yanghua Xiao"
    ],
    "published": "2025-08-18",
    "updated": "2025-08-18",
    "summary": "Recent advances in self-refinement have demonstrated significant potential\nfor improving the outputs of large language models (LLMs) through iterative\nrefinement. However, most existing self-refinement methods rely on a reactive\nprocess with a fixed number of iterations, making it difficult to determine the\noptimal timing and content of refinement based on the evolving generation\ncontext. Inspired by the way humans dynamically refine their thoughts during\nexecution, we propose ProActive Self-Refinement (PASR), a novel method that\nenables LLMs to refine their outputs during the generation process. Unlike\nmethods that regenerate entire responses, PASR proactively decides whether,\nwhen, and how to refine based on the model's internal state and evolving\ncontext. We conduct extensive experiments on a diverse set of 10 tasks to\nevaluate the effectiveness of PASR. Experimental results show that PASR\nsignificantly enhances problem-solving performance. In particular, on Qwen3-8B,\nPASR reduces average token consumption by 41.6 percent compared to standard\ngeneration, while also achieving an 8.2 percent improvement in accuracy. Our\ncode and all baselines used in the paper are available in the GitHub.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "doi": null,
    "pdf_url": "http://arxiv.org/pdf/2508.12903v1",
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CL",
    "links": [
      "http://arxiv.org/abs/2508.12903v1",
      "http://arxiv.org/pdf/2508.12903v1"
    ],
    "pdf": null
  },
  {
    "id": "2508.12868v1",
    "title": "An LLM Agent-Based Complex Semantic Table Annotation Approach",
    "authors": [
      "Yilin Geng",
      "Shujing Wang",
      "Chuan Wang",
      "Keqing He",
      "Yanfei Lv",
      "Ying Wang",
      "Zaiwen Feng",
      "Xiaoying Bai"
    ],
    "published": "2025-08-18",
    "updated": "2025-08-18",
    "summary": "The Semantic Table Annotation (STA) task, which includes Column Type\nAnnotation (CTA) and Cell Entity Annotation (CEA), maps table contents to\nontology entities and plays important roles in various semantic applications.\nHowever, complex tables often pose challenges such as semantic loss of column\nnames or cell values, strict ontological hierarchy requirements, homonyms,\nspelling errors, and abbreviations, which hinder annotation accuracy. To\naddress these issues, this paper proposes an LLM-based agent approach for CTA\nand CEA. We design and implement five external tools with tailored prompts\nbased on the ReAct framework, enabling the STA agent to dynamically select\nsuitable annotation strategies depending on table characteristics. Experiments\nare conducted on the Tough Tables and BiodivTab datasets from the SemTab\nchallenge, which contain the aforementioned challenges. Our method outperforms\nexisting approaches across various metrics. Furthermore, by leveraging\nLevenshtein distance to reduce redundant annotations, we achieve a 70%\nreduction in time costs and a 60% reduction in LLM token usage, providing an\nefficient and cost-effective solution for STA.",
    "categories": [
      "cs.CL",
      "cs.DB"
    ],
    "doi": null,
    "pdf_url": "http://arxiv.org/pdf/2508.12868v1",
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CL",
    "links": [
      "http://arxiv.org/abs/2508.12868v1",
      "http://arxiv.org/pdf/2508.12868v1"
    ],
    "pdf": null
  },
  {
    "id": "2508.12863v1",
    "title": "Word Meanings in Transformer Language Models",
    "authors": [
      "Jumbly Grindrod",
      "Peter Grindrod"
    ],
    "published": "2025-08-18",
    "updated": "2025-08-18",
    "summary": "We investigate how word meanings are represented in the transformer language\nmodels. Specifically, we focus on whether transformer models employ something\nanalogous to a lexical store - where each word has an entry that contains\nsemantic information. To do this, we extracted the token embedding space of\nRoBERTa-base and k-means clustered it into 200 clusters. In our first study, we\nthen manually inspected the resultant clusters to consider whether they are\nsensitive to semantic information. In our second study, we tested whether the\nclusters are sensitive to five psycholinguistic measures: valence,\nconcreteness, iconicity, taboo, and age of acquisition. Overall, our findings\nwere very positive - there is a wide variety of semantic information encoded\nwithin the token embedding space. This serves to rule out certain \"meaning\neliminativist\" hypotheses about how transformer LLMs process semantic\ninformation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "doi": null,
    "pdf_url": "http://arxiv.org/pdf/2508.12863v1",
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CL",
    "links": [
      "http://arxiv.org/abs/2508.12863v1",
      "http://arxiv.org/pdf/2508.12863v1"
    ],
    "pdf": null
  },
  {
    "id": "2508.12854v1",
    "title": "E3RG: Building Explicit Emotion-driven Empathetic Response Generation System with Multimodal Large Language Model",
    "authors": [
      "Ronghao Lin",
      "Shuai Shen",
      "Weipeng Hu",
      "Qiaolin He",
      "Aolin Xiong",
      "Li Huang",
      "Haifeng Hu",
      "Yap-peng Tan"
    ],
    "published": "2025-08-18",
    "updated": "2025-08-18",
    "summary": "Multimodal Empathetic Response Generation (MERG) is crucial for building\nemotionally intelligent human-computer interactions. Although large language\nmodels (LLMs) have improved text-based ERG, challenges remain in handling\nmultimodal emotional content and maintaining identity consistency. Thus, we\npropose E3RG, an Explicit Emotion-driven Empathetic Response Generation System\nbased on multimodal LLMs which decomposes MERG task into three parts:\nmultimodal empathy understanding, empathy memory retrieval, and multimodal\nresponse generation. By integrating advanced expressive speech and video\ngenerative models, E3RG delivers natural, emotionally rich, and\nidentity-consistent responses without extra training. Experiments validate the\nsuperiority of our system on both zero-shot and few-shot settings, securing\nTop-1 position in the Avatar-based Multimodal Empathy Challenge on ACM MM 25.\nOur code is available at https://github.com/RH-Lin/E3RG.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.HC",
      "cs.MM"
    ],
    "doi": null,
    "pdf_url": "http://arxiv.org/pdf/2508.12854v1",
    "comment": "Accepted at ACM MM 2025 Grand Challenge",
    "journal_ref": null,
    "primary_category": "cs.AI",
    "links": [
      "http://arxiv.org/abs/2508.12854v1",
      "http://arxiv.org/pdf/2508.12854v1"
    ],
    "pdf": null
  },
  {
    "id": "2508.12830v1",
    "title": "It takes a village to write a book: Mapping anonymous contributions in Stephen Langton's Quaestiones Theologiae",
    "authors": [
      "Jan Maliszewski"
    ],
    "published": "2025-08-18",
    "updated": "2025-08-18",
    "summary": "While the indirect evidence suggests that already in the early scholastic\nperiod the literary production based on records of oral teaching (so-called\nreportationes) was not uncommon, there are very few sources commenting on the\npractice. This paper details the design of a study applying stylometric\ntechniques of authorship attribution to a collection developed from\nreportationes -- Stephen Langton's Quaestiones Theologiae -- aiming to uncover\nlayers of editorial work and thus validate some hypotheses regarding the\ncollection's formation. Following Camps, Cl\\'erice, and Pinche (2021), I\ndiscuss the implementation of an HTR pipeline and stylometric analysis based on\nthe most frequent words, POS tags, and pseudo-affixes. The proposed study will\noffer two methodological gains relevant to computational research on the\nscholastic tradition: it will directly compare performance on manually composed\nand automatically extracted data, and it will test the validity of\ntransformer-based OCR and automated transcription alignment for workflows\napplied to scholastic Latin corpora. If successful, this study will provide an\neasily reusable template for the exploratory analysis of collaborative literary\nproduction stemming from medieval universities.",
    "categories": [
      "cs.CL"
    ],
    "doi": "10.1017/chr.2025.10004",
    "pdf_url": "http://arxiv.org/pdf/2508.12830v1",
    "comment": null,
    "journal_ref": "Computational Humanities Research , Volume 1 , 2025 , e2",
    "primary_category": "cs.CL",
    "links": [
      "http://dx.doi.org/10.1017/chr.2025.10004",
      "http://arxiv.org/abs/2508.12830v1",
      "http://arxiv.org/pdf/2508.12830v1"
    ],
    "pdf": null
  },
  {
    "id": "2508.12828v1",
    "title": "Context Matters: Incorporating Target Awareness in Conversational Abusive Language Detection",
    "authors": [
      "Raneem Alharthi",
      "Rajwa Alharthi",
      "Aiqi Jiang",
      "Arkaitz Zubiaga"
    ],
    "published": "2025-08-18",
    "updated": "2025-08-18",
    "summary": "Abusive language detection has become an increasingly important task as a\nmeans to tackle this type of harmful content in social media. There has been a\nsubstantial body of research developing models for determining if a social\nmedia post is abusive or not; however, this research has primarily focused on\nexploiting social media posts individually, overlooking additional context that\ncan be derived from surrounding posts. In this study, we look at conversational\nexchanges, where a user replies to an earlier post by another user (the parent\ntweet). We ask: does leveraging context from the parent tweet help determine if\na reply post is abusive or not, and what are the features that contribute the\nmost? We study a range of content-based and account-based features derived from\nthe context, and compare this to the more widely studied approach of only\nlooking at the features from the reply tweet. For a more generalizable study,\nwe test four different classification models on a dataset made of\nconversational exchanges (parent-reply tweet pairs) with replies labeled as\nabusive or not. Our experiments show that incorporating contextual features\nleads to substantial improvements compared to the use of features derived from\nthe reply tweet only, confirming the importance of leveraging context. We\nobserve that, among the features under study, it is especially the\ncontent-based features (what is being posted) that contribute to the\nclassification performance rather than account-based features (who is posting\nit). While using content-based features, it is best to combine a range of\ndifferent features to ensure improved performance over being more selective and\nusing fewer features. Our study provides insights into the development of\ncontextualized abusive language detection models in realistic settings\ninvolving conversations.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "doi": null,
    "pdf_url": "http://arxiv.org/pdf/2508.12828v1",
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CL",
    "links": [
      "http://arxiv.org/abs/2508.12828v1",
      "http://arxiv.org/pdf/2508.12828v1"
    ],
    "pdf": null
  },
  {
    "id": "2508.12819v1",
    "title": "ding-01 :ARG0: An AMR Corpus for Spontaneous French Dialogue",
    "authors": [
      "Jeongwoo Kang",
      "Maria Boritchev",
      "Maximin Coavoux"
    ],
    "published": "2025-08-18",
    "updated": "2025-08-18",
    "summary": "We present our work to build a French semantic corpus by annotating French\ndialogue in Abstract Meaning Representation (AMR). Specifically, we annotate\nthe DinG corpus, consisting of transcripts of spontaneous French dialogues\nrecorded during the board game Catan. As AMR has insufficient coverage of the\ndynamics of spontaneous speech, we extend the framework to better represent\nspontaneous speech and sentence structures specific to French. Additionally, to\nsupport consistent annotation, we provide an annotation guideline detailing\nthese extensions. We publish our corpus under a free license (CC-SA-BY). We\nalso train and evaluate an AMR parser on our data. This model can be used as an\nassistance annotation tool to provide initial annotations that can be refined\nby human annotators. Our work contributes to the development of semantic\nresources for French dialogue.",
    "categories": [
      "cs.CL"
    ],
    "doi": null,
    "pdf_url": "http://arxiv.org/pdf/2508.12819v1",
    "comment": "Accepted at IWCS 2025",
    "journal_ref": null,
    "primary_category": "cs.CL",
    "links": [
      "http://arxiv.org/abs/2508.12819v1",
      "http://arxiv.org/pdf/2508.12819v1"
    ],
    "pdf": null
  },
  {
    "id": "2508.12815v1",
    "title": "Learning to Steer: Input-dependent Steering for Multimodal LLMs",
    "authors": [
      "Jayneel Parekh",
      "Pegah Khayatan",
      "Mustafa Shukor",
      "Arnaud Dapogny",
      "Alasdair Newson",
      "Matthieu Cord"
    ],
    "published": "2025-08-18",
    "updated": "2025-08-18",
    "summary": "Steering has emerged as a practical approach to enable post-hoc guidance of\nLLMs towards enforcing a specific behavior. However, it remains largely\nunderexplored for multimodal LLMs (MLLMs); furthermore, existing steering\ntechniques, such as mean steering, rely on a single steering vector, applied\nindependently of the input query. This paradigm faces limitations when the\ndesired behavior is dependent on the example at hand. For example, a safe\nanswer may consist in abstaining from answering when asked for an illegal\nactivity, or may point to external resources or consultation with an expert\nwhen asked about medical advice. In this paper, we investigate a fine-grained\nsteering that uses an input-specific linear shift. This shift is computed using\ncontrastive input-specific prompting. However, the input-specific prompts\nrequired for this approach are not known at test time. Therefore, we propose to\ntrain a small auxiliary module to predict the input-specific steering vector.\nOur approach, dubbed as L2S (Learn-to-Steer), demonstrates that it reduces\nhallucinations and enforces safety in MLLMs, outperforming other static\nbaselines.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "doi": null,
    "pdf_url": "http://arxiv.org/pdf/2508.12815v1",
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.LG",
    "links": [
      "http://arxiv.org/abs/2508.12815v1",
      "http://arxiv.org/pdf/2508.12815v1"
    ],
    "pdf": null
  },
  {
    "id": "2508.12803v1",
    "title": "When Alignment Hurts: Decoupling Representational Spaces in Multilingual Models",
    "authors": [
      "Ahmed Elshabrawy",
      "Hour Kaing",
      "Haiyue Song",
      "Alham Fikri Aji",
      "Hideki Tanaka",
      "Masao Utiyama",
      "Raj Dabre"
    ],
    "published": "2025-08-18",
    "updated": "2025-08-18",
    "summary": "Alignment with high-resource standard languages is often assumed to aid the\nmodeling of related low-resource varieties. We challenge this assumption by\ndemonstrating that excessive representational entanglement with a dominant\nvariety, such as Modern Standard Arabic (MSA) in relation to Arabic dialects,\ncan actively hinder generative modeling. We present the first comprehensive\ncausal study of this phenomenon by analyzing and directly intervening in the\ninternal representation geometry of large language models (LLMs). Our key\ncontribution is an online variational probing framework that continuously\nestimates the subspace of the standard variety during fine-tuning, enabling\nprojection-based decoupling from this space. While our study uses Arabic as a\ncase due to its unusually rich parallel resources across 25 dialects, the\nbroader motivation is methodological: dialectal MT serves as a controlled proxy\nfor generative tasks where comparable multi-variety corpora are unavailable.\nAcross 25 dialects, our intervention improves generation quality by up to +4.9\nchrF++ and +2.0 on average compared to standard fine-tuning, despite a measured\ntradeoff in standard-language performance. These results provide causal\nevidence that subspace dominance by high-resource varieties can restrict\ngenerative capacity for related varieties. More generally, we unify geometric\nand information-theoretic probing with subspace-level causal interventions,\noffering practical tools for improving generative modeling in closely related\nlanguage families and, more broadly, for controlling representational\nallocation in multilingual and multi-domain LLMs. Code will be released.",
    "categories": [
      "cs.CL"
    ],
    "doi": null,
    "pdf_url": "http://arxiv.org/pdf/2508.12803v1",
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CL",
    "links": [
      "http://arxiv.org/abs/2508.12803v1",
      "http://arxiv.org/pdf/2508.12803v1"
    ],
    "pdf": null
  },
  {
    "id": "2508.12801v1",
    "title": "Maximum Score Routing For Mixture-of-Experts",
    "authors": [
      "Bowen Dong",
      "Yilong Fan",
      "Yutao Sun",
      "Zhenyu Li",
      "Tengyu Pan",
      "Xun Zhou",
      "Jianyong Wang"
    ],
    "published": "2025-08-18",
    "updated": "2025-08-18",
    "summary": "Routing networks in sparsely activated mixture-of-experts (MoE) dynamically\nallocate input tokens to top-k experts through differentiable sparse\ntransformations, enabling scalable model capacity while preserving\ncomputational efficiency. Traditional MoE networks impose an expert capacity\nconstraint to ensure GPU-friendly computation. However, this leads to token\ndropping when capacity is saturated and results in low hardware efficiency due\nto padding in underutilized experts. Removing the capacity constraint, in turn,\ncompromises load balancing and computational efficiency. To address these\nissues, we propose Maximum Score Routing ($\\mathbf{MaxScore}$), a novel MoE\nrouting paradigm that models routing as a minimum-cost maximum-flow problem and\nintegrates a SoftTopk operator. MaxScore resolves the fundamental limitations\nof iterative rerouting and optimal transport formulations, achieving lower\ntraining losses and higher evaluation scores at equivalent FLOPs compared to\nboth constrained and unconstrained baselines. Implementation details and\nexperimental configurations can be obtained from\n$\\href{https://github.com/dongbw18/MaxScore.git}{MaxScore}$.",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "doi": null,
    "pdf_url": "http://arxiv.org/pdf/2508.12801v1",
    "comment": null,
    "journal_ref": "In Findings of the Association for Computational Linguistics: ACL\n  2025, pages 12619-12632, Vienna, Austria",
    "primary_category": "cs.LG",
    "links": [
      "http://arxiv.org/abs/2508.12801v1",
      "http://arxiv.org/pdf/2508.12801v1"
    ],
    "pdf": null
  },
  {
    "id": "2508.12800v1",
    "title": "Atom-Searcher: Enhancing Agentic Deep Research via Fine-Grained Atomic Thought Reward",
    "authors": [
      "Yong Deng",
      "Guoqing Wang",
      "Zhenzhe Ying",
      "Xiaofeng Wu",
      "Jinzhen Lin",
      "Wenwen Xiong",
      "Yuqin Dai",
      "Shuo Yang",
      "Zhanwei Zhang",
      "Qiwen Wang",
      "Yang Qin",
      "Changhua Meng"
    ],
    "published": "2025-08-18",
    "updated": "2025-08-18",
    "summary": "Large language models (LLMs) exhibit remarkable problem-solving abilities,\nbut struggle with complex tasks due to static internal knowledge.\nRetrieval-Augmented Generation (RAG) enhances access to external information,\nyet remains limited in multi-hop reasoning and strategic search due to rigid\nworkflows. Recent advancements in agentic deep research empower LLMs to\nautonomously reason, search, and synthesize information. However, current\napproaches relying on outcome-based reinforcement learning (RL) face critical\nissues such as conflicting gradients and reward sparsity, limiting performance\ngains and training efficiency. To address these, we first propose Atomic\nThought, a novel LLM thinking paradigm that decomposes reasoning into\nfine-grained functional units. These units are supervised by Reasoning Reward\nModels (RRMs), which provide Atomic Thought Rewards (ATR) for fine-grained\nguidance. Building on this, we propose Atom-Searcher, a novel RL framework for\nagentic deep research that integrates Atomic Thought and ATR. Atom-Searcher\nuses a curriculum-inspired reward schedule, prioritizing process-level ATR\nearly and transitioning to outcome rewards, accelerating convergence on\neffective reasoning paths. Experiments on seven benchmarks show consistent\nimprovements over the state-of-the-art. Key advantages include: (1)\nAtom-Searcher scales computation at test-time. (2) Atomic Thought provides\nsupervision anchors for RRMs, bridging deep research tasks and RRMs. (3)\nAtom-Searcher exhibits more interpretable, human-like reasoning patterns.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "doi": null,
    "pdf_url": "http://arxiv.org/pdf/2508.12800v1",
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CL",
    "links": [
      "http://arxiv.org/abs/2508.12800v1",
      "http://arxiv.org/pdf/2508.12800v1"
    ],
    "pdf": null
  },
  {
    "id": "2508.12792v1",
    "title": "Bridging Human and LLM Judgments: Understanding and Narrowing the Gap",
    "authors": [
      "Felipe Maia Polo",
      "Xinhe Wang",
      "Mikhail Yurochkin",
      "Gongjun Xu",
      "Moulinath Banerjee",
      "Yuekai Sun"
    ],
    "published": "2025-08-18",
    "updated": "2025-08-18",
    "summary": "Large language models are increasingly used as judges (LLM-as-a-judge) to\nevaluate model outputs at scale, but their assessments often diverge\nsystematically from human judgments. We present Bridge, a unified statistical\nframework that explicitly bridges human and LLM evaluations under both absolute\nscoring and pairwise comparison paradigms. Bridge posits a latent human\npreference score for each prompt-response pair and models LLM deviations as\nlinear transformations of covariates that capture sources of discrepancies.\nThis offers a simple and principled framework for refining LLM ratings and\ncharacterizing systematic discrepancies between humans and LLMs. We provide an\nefficient fitting algorithm with asymptotic guarantees for statistical\ninference. Using six LLM judges and two benchmarks (BigGen Bench and Chatbot\nArena), Bridge achieves higher agreement with human ratings (accuracy,\ncalibration, and KL divergence) and exposes systematic human-LLM gaps.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ML"
    ],
    "doi": null,
    "pdf_url": "http://arxiv.org/pdf/2508.12792v1",
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.LG",
    "links": [
      "http://arxiv.org/abs/2508.12792v1",
      "http://arxiv.org/pdf/2508.12792v1"
    ],
    "pdf": null
  },
  {
    "id": "2508.12790v1",
    "title": "Reinforcement Learning with Rubric Anchors",
    "authors": [
      "Zenan Huang",
      "Yihong Zhuang",
      "Guoshan Lu",
      "Zeyu Qin",
      "Haokai Xu",
      "Tianyu Zhao",
      "Ru Peng",
      "Jiaqi Hu",
      "Zhanming Shen",
      "Xiaomeng Hu",
      "Xijun Gu",
      "Peiyi Tu",
      "Jiaxin Liu",
      "Wenyu Chen",
      "Yuzhuo Fu",
      "Zhiting Fan",
      "Yanmei Gu",
      "Yuanyuan Wang",
      "Zhengkai Yang",
      "Jianguo Li",
      "Junbo Zhao"
    ],
    "published": "2025-08-18",
    "updated": "2025-08-18",
    "summary": "Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a\npowerful paradigm for enhancing Large Language Models (LLMs), exemplified by\nthe success of OpenAI's o-series. In RLVR, rewards are derived from verifiable\nsignals-such as passing unit tests in code generation or matching correct\nanswers in mathematical reasoning. While effective, this requirement largely\nconfines RLVR to domains with automatically checkable outcomes. To overcome\nthis, we extend the RLVR paradigm to open-ended tasks by integrating\nrubric-based rewards, where carefully designed rubrics serve as structured,\nmodel-interpretable criteria for automatic scoring of subjective outputs. We\nconstruct, to our knowledge, the largest rubric reward system to date, with\nover 10,000 rubrics from humans, LLMs, or a hybrid human-LLM collaboration.\nImplementing rubric-based RL is challenging; we tackle these issues with a\nclear framework and present an open-sourced Qwen-30B-A3B model with notable\ngains: 1) With only 5K+ samples, our system improves by +5.2% on open-ended\nbenchmarks (especially humanities), outperforming a 671B DeepSeek-V3 model by\n+2.4%, while preserving general and reasoning abilities. 2) Our method provides\nfine-grained stylistic control, using rubrics as anchors to mitigate the\n\"AI-like\" tone and produce more human-like, expressive responses. We share key\nlessons in rubric construction, data selection, and training, and discuss\nlimitations and future releases.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "doi": null,
    "pdf_url": "http://arxiv.org/pdf/2508.12790v1",
    "comment": "technical report",
    "journal_ref": null,
    "primary_category": "cs.AI",
    "links": [
      "http://arxiv.org/abs/2508.12790v1",
      "http://arxiv.org/pdf/2508.12790v1"
    ],
    "pdf": null
  },
  {
    "id": "2508.12778v1",
    "title": "HeteroRAG: A Heterogeneous Retrieval-Augmented Generation Framework for Medical Vision Language Tasks",
    "authors": [
      "Zhe Chen",
      "Yusheng Liao",
      "Shuyang Jiang",
      "Zhiyuan Zhu",
      "Haolin Li",
      "Yanfeng Wang",
      "Yu Wang"
    ],
    "published": "2025-08-18",
    "updated": "2025-08-18",
    "summary": "Medical large vision-language Models (Med-LVLMs) have shown promise in\nclinical applications but suffer from factual inaccuracies and unreliable\noutputs, posing risks in real-world diagnostics. While retrieval-augmented\ngeneration has emerged as a potential solution, current medical multimodal RAG\nsystems are unable to perform effective retrieval across heterogeneous sources.\nThe irrelevance of retrieved reports affects the factuality of analysis, while\ninsufficient knowledge affects the credibility of clinical decision-making. To\nbridge the gap, we construct MedAtlas, which includes extensive multimodal\nreport repositories and diverse text corpora. Based on it, we present\nHeteroRAG, a novel framework that enhances Med-LVLMs through heterogeneous\nknowledge sources. The framework introduces Modality-specific CLIPs for\neffective report retrieval and a Multi-corpora Query Generator for dynamically\nconstructing queries for diverse corpora. Incorporating knowledge from such\nmultifaceted sources, Med-LVLM is then trained with Heterogeneous Knowledge\nPreference Tuning to achieve cross-modality and multi-source knowledge\nalignment. Extensive experiments across 12 datasets and 3 modalities\ndemonstrate that the proposed HeteroRAG achieves state-of-the-art performance\nin most medical vision language benchmarks, significantly improving factual\naccuracy and reliability of Med-LVLMs.",
    "categories": [
      "cs.CL"
    ],
    "doi": null,
    "pdf_url": "http://arxiv.org/pdf/2508.12778v1",
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CL",
    "links": [
      "http://arxiv.org/abs/2508.12778v1",
      "http://arxiv.org/pdf/2508.12778v1"
    ],
    "pdf": null
  },
  {
    "id": "2508.12774v1",
    "title": "From SALAMANDRA to SALAMANDRATA: BSC Submission for WMT25 General Machine Translation Shared Task",
    "authors": [
      "Javier Garcia Gilabert",
      "Xixian Liao",
      "Severino Da Dalt",
      "Ella Bohman",
      "Audrey Mash",
      "Francesca De Luca Fornaciari",
      "Irene Baucells",
      "Joan Llop",
      "Miguel Claramunt Argote",
      "Carlos Escolano",
      "Maite Melero"
    ],
    "published": "2025-08-18",
    "updated": "2025-08-18",
    "summary": "In this paper, we present the SALAMANDRATA family of models, an improved\niteration of SALAMANDRA LLMs (Gonzalez-Agirre et al., 2025) specifically\ntrained to achieve strong performance in translation-related tasks for 38\nEuropean languages. SALAMANDRATA comes in two scales: 2B and 7B parameters. For\nboth versions, we applied the same training recipe with a first step of\ncontinual pre-training on parallel data, and a second step of supervised\nfine-tuning on high-quality instructions. The BSC submission to the WMT25\nGeneral Machine Translation shared task is based on the 7B variant of\nSALAMANDRATA. We first adapted the model vocabulary to support the additional\nnon-European languages included in the task. This was followed by a second\nphase of continual pre-training and supervised fine-tuning, carefully designed\nto optimize performance across all translation directions for this year's\nshared task. For decoding, we employed two quality-aware strategies: Minimum\nBayes Risk Decoding and Tuned Re-ranking using COMET and COMET-KIWI\nrespectively. We publicly release both the 2B and 7B versions of SALAMANDRATA,\nalong with the newer SALAMANDRATA-V2 model, on Hugging Face1",
    "categories": [
      "cs.CL"
    ],
    "doi": null,
    "pdf_url": "http://arxiv.org/pdf/2508.12774v1",
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CL",
    "links": [
      "http://arxiv.org/abs/2508.12774v1",
      "http://arxiv.org/pdf/2508.12774v1"
    ],
    "pdf": null
  },
  {
    "id": "2508.12769v1",
    "title": "CRED-SQL: Enhancing Real-world Large Scale Database Text-to-SQL Parsing through Cluster Retrieval and Execution Description",
    "authors": [
      "Shaoming Duan",
      "Zirui Wang",
      "Chuanyi Liu",
      "Zhibin Zhu",
      "Yuhao Zhang",
      "Peiyi Han",
      "Liang Yan",
      "Zewu Penge"
    ],
    "published": "2025-08-18",
    "updated": "2025-08-18",
    "summary": "Recent advances in large language models (LLMs) have significantly improved\nthe accuracy of Text-to-SQL systems. However, a critical challenge remains: the\nsemantic mismatch between natural language questions (NLQs) and their\ncorresponding SQL queries. This issue is exacerbated in large-scale databases,\nwhere semantically similar attributes hinder schema linking and semantic drift\nduring SQL generation, ultimately reducing model accuracy. To address these\nchallenges, we introduce CRED-SQL, a framework designed for large-scale\ndatabases that integrates Cluster Retrieval and Execution Description. CRED-SQL\nfirst performs cluster-based large-scale schema retrieval to pinpoint the\ntables and columns most relevant to a given NLQ, alleviating schema mismatch.\nIt then introduces an intermediate natural language representation-Execution\nDescription Language (EDL)-to bridge the gap between NLQs and SQL. This\nreformulation decomposes the task into two stages: Text-to-EDL and EDL-to-SQL,\nleveraging LLMs' strong general reasoning capabilities while reducing semantic\ndeviation. Extensive experiments on two large-scale, cross-domain\nbenchmarks-SpiderUnion and BirdUnion-demonstrate that CRED-SQL achieves new\nstate-of-the-art (SOTA) performance, validating its effectiveness and\nscalability. Our code is available at https://github.com/smduan/CRED-SQL.git",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "doi": null,
    "pdf_url": "http://arxiv.org/pdf/2508.12769v1",
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CL",
    "links": [
      "http://arxiv.org/abs/2508.12769v1",
      "http://arxiv.org/pdf/2508.12769v1"
    ],
    "pdf": null
  },
  {
    "id": "2508.12733v1",
    "title": "LinguaSafe: A Comprehensive Multilingual Safety Benchmark for Large Language Models",
    "authors": [
      "Zhiyuan Ning",
      "Tianle Gu",
      "Jiaxin Song",
      "Shixin Hong",
      "Lingyu Li",
      "Huacan Liu",
      "Jie Li",
      "Yixu Wang",
      "Meng Lingyu",
      "Yan Teng",
      "Yingchun Wang"
    ],
    "published": "2025-08-18",
    "updated": "2025-08-18",
    "summary": "The widespread adoption and increasing prominence of large language models\n(LLMs) in global technologies necessitate a rigorous focus on ensuring their\nsafety across a diverse range of linguistic and cultural contexts. The lack of\na comprehensive evaluation and diverse data in existing multilingual safety\nevaluations for LLMs limits their effectiveness, hindering the development of\nrobust multilingual safety alignment. To address this critical gap, we\nintroduce LinguaSafe, a comprehensive multilingual safety benchmark crafted\nwith meticulous attention to linguistic authenticity. The LinguaSafe dataset\ncomprises 45k entries in 12 languages, ranging from Hungarian to Malay. Curated\nusing a combination of translated, transcreated, and natively-sourced data, our\ndataset addresses the critical need for multilingual safety evaluations of\nLLMs, filling the void in the safety evaluation of LLMs across diverse\nunder-represented languages from Hungarian to Malay. LinguaSafe presents a\nmultidimensional and fine-grained evaluation framework, with direct and\nindirect safety assessments, including further evaluations for oversensitivity.\nThe results of safety and helpfulness evaluations vary significantly across\ndifferent domains and different languages, even in languages with similar\nresource levels. Our benchmark provides a comprehensive suite of metrics for\nin-depth safety evaluation, underscoring the critical importance of thoroughly\nassessing multilingual safety in LLMs to achieve more balanced safety\nalignment. Our dataset and code are released to the public to facilitate\nfurther research in the field of multilingual LLM safety.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "doi": null,
    "pdf_url": "http://arxiv.org/pdf/2508.12733v1",
    "comment": "7pages, 5 figures",
    "journal_ref": null,
    "primary_category": "cs.CL",
    "links": [
      "http://arxiv.org/abs/2508.12733v1",
      "http://arxiv.org/pdf/2508.12733v1"
    ],
    "pdf": null
  },
  {
    "id": "2508.12726v1",
    "title": "DESIGNER: Design-Logic-Guided Multidisciplinary Data Synthesis for LLM Reasoning",
    "authors": [
      "Weize Liu",
      "Yongchi Zhao",
      "Yijia Luo",
      "Mingyu Xu",
      "Jiaheng Liu",
      "Yanan Li",
      "Xiguo Hu",
      "Yuchi Xu",
      "Wenbo Su",
      "Bo Zheng"
    ],
    "published": "2025-08-18",
    "updated": "2025-08-18",
    "summary": "Large language models (LLMs) have achieved remarkable success in many natural\nlanguage tasks but still struggle with complex, multi-step reasoning,\nparticularly across diverse disciplines. Existing reasoning datasets often\neither lack disciplinary breadth or the structural depth necessary to elicit\nrobust reasoning behaviors. We propose DESIGNER: a DESIGN-logic-guidEd\nReasoning data synthesis pipeline that leverages naturally available, extensive\nraw documents (book corpus and web corpus) to generate multidisciplinary\nchallenging questions. A core innovation of our approach is the introduction of\na Design Logic concept, which mimics the question-creation process of human\neducators. We use LLMs to reverse-engineer and abstract over 120,000 design\nlogics from existing questions across various disciplines. By matching these\ndesign logics with disciplinary source materials, we are able to create\nreasoning questions that far surpass the difficulty and diversity of existing\ndatasets. Based on this pipeline, we synthesized two large-scale reasoning\ndatasets that span 75 disciplines: Design-Logic-Reasoning-Book (DLR-Book),\ncontaining 3.04 million challenging questions synthesized from the book corpus,\nand Design-Logic-Reasoning-Web (DLR-Web), with 1.66 million challenging\nquestions from the web corpus. Our data analysis demonstrates that the\nquestions synthesized by our method exhibit substantially greater difficulty\nand diversity than those in the baseline datasets. We validate the\neffectiveness of these datasets by conducting SFT experiments on the\nQwen3-8B-Base and Qwen3-4B-Base models. The results show that our dataset\nsignificantly outperforms existing multidisciplinary datasets of the same\nvolume. Training with the full datasets further enables the models to surpass\nthe multidisciplinary reasoning performance of the official Qwen3-8B and\nQwen3-4B models.",
    "categories": [
      "cs.CL"
    ],
    "doi": null,
    "pdf_url": "http://arxiv.org/pdf/2508.12726v1",
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CL",
    "links": [
      "http://arxiv.org/abs/2508.12726v1",
      "http://arxiv.org/pdf/2508.12726v1"
    ],
    "pdf": null
  },
  {
    "id": "2508.12685v1",
    "title": "ToolACE-MT: Non-Autoregressive Generation for Agentic Multi-Turn Interaction",
    "authors": [
      "Xingshan Zeng",
      "Weiwen Liu",
      "Lingzhi Wang",
      "Liangyou Li",
      "Fei Mi",
      "Yasheng Wang",
      "Lifeng Shang",
      "Xin Jiang",
      "Qun Liu"
    ],
    "published": "2025-08-18",
    "updated": "2025-08-18",
    "summary": "Agentic task-solving with Large Language Models (LLMs) requires multi-turn,\nmulti-step interactions, often involving complex function calls and dynamic\nuser-agent exchanges. Existing simulation-based data generation methods for\nsuch scenarios rely heavily on costly autoregressive interactions between\nmultiple LLM agents, thereby limiting real-world performance of agentic tasks.\nIn this paper, we propose a novel Non-Autoregressive Iterative Generation\nframework, called ToolACE-MT, for constructing high-quality multi-turn agentic\ndialogues. ToolACE-MT generates full conversational trajectories through three\nstages: coarse-grained initialization, iterative refinement, and offline\nverification. The initialization phase builds a structurally complete yet\nsemantically coarse dialogue skeleton; the iterative refinement phase\nintroduces realistic complexities and continued refinement via mask-and-fill\noperations; and the offline verification phase ensures correctness and\ncoherence via rule- and model-based checks. Experiments demonstrate that\nToolACE-MT enables efficient, effective and generalizable agentic data\ngeneration, offering a new paradigm for high-quality data construction in\ntool-augmented LLM scenarios.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "doi": null,
    "pdf_url": "http://arxiv.org/pdf/2508.12685v1",
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CL",
    "links": [
      "http://arxiv.org/abs/2508.12685v1",
      "http://arxiv.org/pdf/2508.12685v1"
    ],
    "pdf": null
  },
  {
    "id": "2508.12680v1",
    "title": "Vision-G1: Towards General Vision Language Reasoning with Multi-Domain Data Curation",
    "authors": [
      "Yuheng Zha",
      "Kun Zhou",
      "Yujia Wu",
      "Yushu Wang",
      "Jie Feng",
      "Zhi Xu",
      "Shibo Hao",
      "Zhengzhong Liu",
      "Eric P. Xing",
      "Zhiting Hu"
    ],
    "published": "2025-08-18",
    "updated": "2025-08-18",
    "summary": "Despite their success, current training pipelines for reasoning VLMs focus on\na limited range of tasks, such as mathematical and logical reasoning. As a\nresult, these models face difficulties in generalizing their reasoning\ncapabilities to a wide range of domains, primarily due to the scarcity of\nreadily available and verifiable reward data beyond these narrowly defined\nareas. Moreover, integrating data from multiple domains is challenging, as the\ncompatibility between domain-specific datasets remains uncertain. To address\nthese limitations, we build a comprehensive RL-ready visual reasoning dataset\nfrom 46 data sources across 8 dimensions, covering a wide range of tasks such\nas infographic, mathematical, spatial, cross-image, graphic user interface,\nmedical, common sense and general science. We propose an influence function\nbased data selection and difficulty based filtering strategy to identify\nhigh-quality training samples from this dataset. Subsequently, we train the\nVLM, referred to as Vision-G1, using multi-round RL with a data curriculum to\niteratively improve its visual reasoning capabilities. Our model achieves\nstate-of-the-art performance across various visual reasoning benchmarks,\noutperforming similar-sized VLMs and even proprietary models like GPT-4o and\nGemini-1.5 Flash. The model, code and dataset are publicly available at\nhttps://github.com/yuh-zha/Vision-G1.",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "doi": null,
    "pdf_url": "http://arxiv.org/pdf/2508.12680v1",
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV",
    "links": [
      "http://arxiv.org/abs/2508.12680v1",
      "http://arxiv.org/pdf/2508.12680v1"
    ],
    "pdf": null
  },
  {
    "id": "2508.12669v1",
    "title": "Leveraging Large Language Models for Predictive Analysis of Human Misery",
    "authors": [
      "Bishanka Seal",
      "Rahul Seetharaman",
      "Aman Bansal",
      "Abhilash Nandy"
    ],
    "published": "2025-08-18",
    "updated": "2025-08-18",
    "summary": "This study investigates the use of Large Language Models (LLMs) for\npredicting human-perceived misery scores from natural language descriptions of\nreal-world scenarios. The task is framed as a regression problem, where the\nmodel assigns a scalar value from 0 to 100 to each input statement. We evaluate\nmultiple prompting strategies, including zero-shot, fixed-context few-shot, and\nretrieval-based prompting using BERT sentence embeddings. Few-shot approaches\nconsistently outperform zero-shot baselines, underscoring the value of\ncontextual examples in affective prediction. To move beyond static evaluation,\nwe introduce the \"Misery Game Show\", a novel gamified framework inspired by a\ntelevision format. It tests LLMs through structured rounds involving ordinal\ncomparison, binary classification, scalar estimation, and feedback-driven\nreasoning. This setup enables us to assess not only predictive accuracy but\nalso the model's ability to adapt based on corrective feedback. The gamified\nevaluation highlights the broader potential of LLMs in dynamic emotional\nreasoning tasks beyond standard regression. Code and data link:\nhttps://github.com/abhi1nandy2/Misery_Data_Exps_GitHub",
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "doi": null,
    "pdf_url": "http://arxiv.org/pdf/2508.12669v1",
    "comment": "14 pages, 4 tables",
    "journal_ref": null,
    "primary_category": "cs.CL",
    "links": [
      "http://arxiv.org/abs/2508.12669v1",
      "http://arxiv.org/pdf/2508.12669v1"
    ],
    "pdf": null
  },
  {
    "id": "2508.12662v1",
    "title": "Breaking Language Barriers: Equitable Performance in Multilingual Language Models",
    "authors": [
      "Tanay Nagar",
      "Grigorii Khvatskii",
      "Anna Sokol",
      "Nitesh V. Chawla"
    ],
    "published": "2025-08-18",
    "updated": "2025-08-18",
    "summary": "Cutting-edge LLMs have emerged as powerful tools for multilingual\ncommunication and understanding. However, LLMs perform worse in Common Sense\nReasoning (CSR) tasks when prompted in low-resource languages (LRLs) like Hindi\nor Swahili compared to high-resource languages (HRLs) like English. Equalizing\nthis inconsistent access to quality LLM outputs is crucial to ensure fairness\nfor speakers of LRLs and across diverse linguistic communities. In this paper,\nwe propose an approach to bridge this gap in LLM performance. Our approach\ninvolves fine-tuning an LLM on synthetic code-switched text generated using\ncontrolled language-mixing methods. We empirically demonstrate that fine-tuning\nLLMs on synthetic code-switched datasets leads to substantial improvements in\nLRL model performance while preserving or enhancing performance in HRLs.\nAdditionally, we present a new dataset of synthetic code-switched text derived\nfrom the CommonSenseQA dataset, featuring three distinct language ratio\nconfigurations.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "doi": null,
    "pdf_url": "http://arxiv.org/pdf/2508.12662v1",
    "comment": "Accepted as a non-archival work-in-progress paper at the NAACL 2025\n  Student Research Workshop",
    "journal_ref": null,
    "primary_category": "cs.CL",
    "links": [
      "http://arxiv.org/abs/2508.12662v1",
      "http://arxiv.org/pdf/2508.12662v1"
    ],
    "pdf": null
  },
  {
    "id": "2508.12632v1",
    "title": "Prompt-Induced Linguistic Fingerprints for LLM-Generated Fake News Detection",
    "authors": [
      "Chi Wang",
      "Min Gao",
      "Zongwei Wang",
      "Junwei Yin",
      "Kai Shu",
      "Chenghua Lin"
    ],
    "published": "2025-08-18",
    "updated": "2025-08-18",
    "summary": "With the rapid development of large language models, the generation of fake\nnews has become increasingly effortless, posing a growing societal threat and\nunderscoring the urgent need for reliable detection methods. Early efforts to\nidentify LLM-generated fake news have predominantly focused on the textual\ncontent itself; however, because much of that content may appear coherent and\nfactually consistent, the subtle traces of falsification are often difficult to\nuncover. Through distributional divergence analysis, we uncover prompt-induced\nlinguistic fingerprints: statistically distinct probability shifts between\nLLM-generated real and fake news when maliciously prompted. Based on this\ninsight, we propose a novel method named Linguistic Fingerprints Extraction\n(LIFE). By reconstructing word-level probability distributions, LIFE can find\ndiscriminative patterns that facilitate the detection of LLM-generated fake\nnews. To further amplify these fingerprint patterns, we also leverage\nkey-fragment techniques that accentuate subtle linguistic differences, thereby\nimproving detection reliability. Our experiments show that LIFE achieves\nstate-of-the-art performance in LLM-generated fake news and maintains high\nperformance in human-written fake news. The code and data are available at\nhttps://anonymous.4open.science/r/LIFE-E86A.",
    "categories": [
      "cs.CL"
    ],
    "doi": null,
    "pdf_url": "http://arxiv.org/pdf/2508.12632v1",
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CL",
    "links": [
      "http://arxiv.org/abs/2508.12632v1",
      "http://arxiv.org/pdf/2508.12632v1"
    ],
    "pdf": null
  },
  {
    "id": "2508.12631v1",
    "title": "Beyond GPT-5: Making LLMs Cheaper and Better via Performance-Efficiency Optimized Routing",
    "authors": [
      "Yiqun Zhang",
      "Hao Li",
      "Jianhao Chen",
      "Hangfan Zhang",
      "Peng Ye",
      "Lei Bai",
      "Shuyue Hu"
    ],
    "published": "2025-08-18",
    "updated": "2025-08-18",
    "summary": "Balancing performance and efficiency is a central challenge in large language\nmodel (LLM) advancement. GPT-5 addresses this with test-time routing,\ndynamically assigning queries to either an efficient or a high-capacity model\nduring inference. In this work, we present Avengers-Pro, a test-time routing\nframework that ensembles LLMs of varying capacities and efficiencies, providing\na unified solution for all performance-efficiency tradeoffs. The Avengers-Pro\nembeds and clusters incoming queries, then routes each to the most suitable\nmodel based on a performance-efficiency score. Across 6 challenging benchmarks\nand 8 leading models -- including GPT-5-medium, Gemini-2.5-pro, and\nClaude-opus-4.1 -- Avengers-Pro achieves state-of-the-art results: by varying a\nperformance-efficiency trade-off parameter, it can surpass the strongest single\nmodel (GPT-5-medium) by +7% in average accuracy. Moreover, it can match the\naverage accuracy of the strongest single model at 27% lower cost, and reach\n~90% of that performance at 63% lower cost. Last but not least, it achieves a\nPareto frontier, consistently yielding the highest accuracy for any given cost,\nand the lowest cost for any given accuracy, among all single models. Code is\navailable at https://github.com/ZhangYiqun018/AvengersPro.",
    "categories": [
      "cs.CL"
    ],
    "doi": null,
    "pdf_url": "http://arxiv.org/pdf/2508.12631v1",
    "comment": "Ongoing work",
    "journal_ref": null,
    "primary_category": "cs.CL",
    "links": [
      "http://arxiv.org/abs/2508.12631v1",
      "http://arxiv.org/pdf/2508.12631v1"
    ],
    "pdf": null
  },
  {
    "id": "2508.12630v1",
    "title": "Semantic Anchoring in Agentic Memory: Leveraging Linguistic Structures for Persistent Conversational Context",
    "authors": [
      "Maitreyi Chatterjee",
      "Devansh Agarwal"
    ],
    "published": "2025-08-18",
    "updated": "2025-08-18",
    "summary": "Large Language Models (LLMs) have demonstrated impressive fluency and task\ncompetence in conversational settings. However, their effectiveness in\nmulti-session and long-term interactions is hindered by limited memory\npersistence. Typical retrieval-augmented generation (RAG) systems store\ndialogue history as dense vectors, which capture semantic similarity but\nneglect finer linguistic structures such as syntactic dependencies, discourse\nrelations, and coreference links. We propose Semantic Anchoring, a hybrid\nagentic memory architecture that enriches vector-based storage with explicit\nlinguistic cues to improve recall of nuanced, context-rich exchanges. Our\napproach combines dependency parsing, discourse relation tagging, and\ncoreference resolution to create structured memory entries. Experiments on\nadapted long-term dialogue datasets show that semantic anchoring improves\nfactual recall and discourse coherence by up to 18% over strong RAG baselines.\nWe further conduct ablation studies, human evaluations, and error analysis to\nassess robustness and interpretability.",
    "categories": [
      "cs.CL"
    ],
    "doi": null,
    "pdf_url": "http://arxiv.org/pdf/2508.12630v1",
    "comment": "Paper is currently in peer review",
    "journal_ref": null,
    "primary_category": "cs.CL",
    "links": [
      "http://arxiv.org/abs/2508.12630v1",
      "http://arxiv.org/pdf/2508.12630v1"
    ],
    "pdf": null
  },
  {
    "id": "2508.12611v1",
    "title": "An LLM + ASP Workflow for Joint Entity-Relation Extraction",
    "authors": [
      "Trang Tran",
      "Trung Hoang Le",
      "Huiping Cao",
      "Tran Cao Son"
    ],
    "published": "2025-08-18",
    "updated": "2025-08-18",
    "summary": "Joint entity-relation extraction (JERE) identifies both entities and their\nrelationships simultaneously. Traditional machine-learning based approaches to\nperforming this task require a large corpus of annotated data and lack the\nability to easily incorporate domain specific information in the construction\nof the model. Therefore, creating a model for JERE is often labor intensive,\ntime consuming, and elaboration intolerant. In this paper, we propose\nharnessing the capabilities of generative pretrained large language models\n(LLMs) and the knowledge representation and reasoning capabilities of Answer\nSet Programming (ASP) to perform JERE. We present a generic workflow for JERE\nusing LLMs and ASP. The workflow is generic in the sense that it can be applied\nfor JERE in any domain. It takes advantage of LLM's capability in natural\nlanguage understanding in that it works directly with unannotated text. It\nexploits the elaboration tolerant feature of ASP in that no modification of its\ncore program is required when additional domain specific knowledge, in the form\nof type specifications, is found and needs to be used. We demonstrate the\nusefulness of the proposed workflow through experiments with limited training\ndata on three well-known benchmarks for JERE. The results of our experiments\nshow that the LLM + ASP workflow is better than state-of-the-art JERE systems\nin several categories with only 10\\% of training data. It is able to achieve a\n2.5 times (35\\% over 15\\%) improvement in the Relation Extraction task for the\nSciERC corpus, one of the most difficult benchmarks.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "I.2.7; F.4.1"
    ],
    "doi": null,
    "pdf_url": "http://arxiv.org/pdf/2508.12611v1",
    "comment": "13 pages, 1 figure, Accepted as Technical Communication, 41st\n  International Conference on Logic Programming",
    "journal_ref": null,
    "primary_category": "cs.AI",
    "links": [
      "http://arxiv.org/abs/2508.12611v1",
      "http://arxiv.org/pdf/2508.12611v1"
    ],
    "pdf": null
  },
  {
    "id": "2508.12591v1",
    "title": "Beyond Modality Limitations: A Unified MLLM Approach to Automated Speaking Assessment with Effective Curriculum Learning",
    "authors": [
      "Yu-Hsuan Fang",
      "Tien-Hong Lo",
      "Yao-Ting Sung",
      "Berlin Chen"
    ],
    "published": "2025-08-18",
    "updated": "2025-08-18",
    "summary": "Traditional Automated Speaking Assessment (ASA) systems exhibit inherent\nmodality limitations: text-based approaches lack acoustic information while\naudio-based methods miss semantic context. Multimodal Large Language Models\n(MLLM) offer unprecedented opportunities for comprehensive ASA by\nsimultaneously processing audio and text within unified frameworks. This paper\npresents a very first systematic study of MLLM for comprehensive ASA,\ndemonstrating the superior performance of MLLM across the aspects of content\nand language use . However, assessment on the delivery aspect reveals unique\nchallenges, which is deemed to require specialized training strategies. We thus\npropose Speech-First Multimodal Training (SFMT), leveraging a curriculum\nlearning principle to establish more robust modeling foundations of speech\nbefore cross-modal synergetic fusion. A series of experiments on a benchmark\ndataset show MLLM-based systems can elevate the holistic assessment performance\nfrom a PCC value of 0.783 to 0.846. In particular, SFMT excels in the\nevaluation of the delivery aspect, achieving an absolute accuracy improvement\nof 4% over conventional training approaches, which also paves a new avenue for\nASA.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SD"
    ],
    "doi": null,
    "pdf_url": "http://arxiv.org/pdf/2508.12591v1",
    "comment": "Accepted at IEEE ASRU 2025",
    "journal_ref": null,
    "primary_category": "cs.CL",
    "links": [
      "http://arxiv.org/abs/2508.12591v1",
      "http://arxiv.org/pdf/2508.12591v1"
    ],
    "pdf": null
  },
  {
    "id": "2508.12574v1",
    "title": "Insight Rumors: A Novel Textual Rumor Locating and Marking Model Leveraging Att_BiMamba2 Network",
    "authors": [
      "Bin Ma",
      "Yifei Zhang",
      "Yongjin Xian",
      "Qi Li",
      "Linna Zhou",
      "Gongxun Miao"
    ],
    "published": "2025-08-18",
    "updated": "2025-08-18",
    "summary": "With the development of social media networks, rumor detection models have\nattracted more and more attention. Whereas, these models primarily focus on\nclassifying contexts as rumors or not, lacking the capability to locate and\nmark specific rumor content. To address this limitation, this paper proposes a\nnovel rumor detection model named Insight Rumors to locate and mark rumor\ncontent within textual data. Specifically, we propose the Bidirectional Mamba2\nNetwork with Dot-Product Attention (Att_BiMamba2), a network that constructs a\nbidirectional Mamba2 model and applies dot-product attention to weight and\ncombine the outputs from both directions, thereby enhancing the representation\nof high-dimensional rumor features. Simultaneously, a Rumor Locating and\nMarking module is designed to locate and mark rumors. The module constructs a\nskip-connection network to project high-dimensional rumor features onto\nlow-dimensional label features. Moreover, Conditional Random Fields (CRF) is\nemployed to impose strong constraints on the output label features, ensuring\naccurate rumor content location. Additionally, a labeled dataset for rumor\nlocating and marking is constructed, with the effectiveness of the proposed\nmodel is evaluated through comprehensive experiments. Extensive experiments\nindicate that the proposed scheme not only detects rumors accurately but also\nlocates and marks them in context precisely, outperforming state-of-the-art\nschemes that can only discriminate rumors roughly.",
    "categories": [
      "cs.SI",
      "cs.CL"
    ],
    "doi": null,
    "pdf_url": "http://arxiv.org/pdf/2508.12574v1",
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.SI",
    "links": [
      "http://arxiv.org/abs/2508.12574v1",
      "http://arxiv.org/pdf/2508.12574v1"
    ],
    "pdf": null
  },
  {
    "id": "2508.12535v1",
    "title": "CorrSteer: Steering Improves Task Performance and Safety in LLMs through Correlation-based Sparse Autoencoder Feature Selection",
    "authors": [
      "Seonglae Cho",
      "Zekun Wu",
      "Adriano Koshiyama"
    ],
    "published": "2025-08-18",
    "updated": "2025-08-18",
    "summary": "Sparse Autoencoders (SAEs) can extract interpretable features from large\nlanguage models (LLMs) without supervision. However, their effectiveness in\ndownstream steering tasks is limited by the requirement for contrastive\ndatasets or large activation storage. To address these limitations, we propose\nCorrSteer, which selects features by correlating sample correctness with SAE\nactivations from generated tokens at inference time. This approach uses only\ninference-time activations to extract more relevant features, thereby avoiding\nspurious correlations. It also obtains steering coefficients from average\nactivations, automating the entire pipeline. Our method shows improved task\nperformance on QA, bias mitigation, jailbreaking prevention, and reasoning\nbenchmarks on Gemma 2 2B and LLaMA 3.1 8B, notably achieving a +4.1%\nimprovement in MMLU performance and a +22.9% improvement in HarmBench with only\n4000 samples. Selected features demonstrate semantically meaningful patterns\naligned with each task's requirements, revealing the underlying capabilities\nthat drive performance. Our work establishes correlationbased selection as an\neffective and scalable approach for automated SAE steering across language\nmodel applications.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "doi": null,
    "pdf_url": "http://arxiv.org/pdf/2508.12535v1",
    "comment": "42 pages, 9 tables",
    "journal_ref": null,
    "primary_category": "cs.CL",
    "links": [
      "http://arxiv.org/abs/2508.12535v1",
      "http://arxiv.org/pdf/2508.12535v1"
    ],
    "pdf": null
  }
]